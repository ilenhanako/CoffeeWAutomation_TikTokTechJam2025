# ğŸ“¸ YOLO Detection Service

A FastAPI microservice that powersÂ **vision-based element detection**Â for theÂ AutoGUI_TecJam2025 Mobile UI Automation Framework.

It usesÂ **Roboflow YOLO workflows**Â andÂ **intent â†’ class mappings**Â to locate UI elements in screenshots.

---

## ğŸš€ Features

- **Intent to UI Mapping**
    
    Natural-language queries likeÂ `"tap like button"`Â are mapped to YOLO classes (`like`).
    
- **YOLO Inference via Roboflow**
    
    Runs workflows hosted onÂ [Roboflow](https://roboflow.com/)Â viaÂ `inference_sdk`.
    
- **Fuzzy Matching**
    
    Automatically expands queries to the closest YOLO classes (e.g.,Â `"magnifying"`Â â†’Â `"search"`).
    
- **REST API**
    - `GET /health`Â â†’ health check
    - `POST /predict`Â â†’ detect UI element from image + query

---

## âš™ï¸ Prerequisites

- Python 3.9+
- A validÂ **Roboflow API key**Â (`ROBOFLOW_API_KEY`)
- Installed dependencies (`requirements.txt`Â inÂ `test-automation/`)

---

## ğŸ”§ Installation

```bash
cd test-automation/yolo_detection
python -m venv venv
source venv/bin/activate

pip install --upgrade pip setuptools wheel
pip install fastapi uvicorn pillow inference-sdk rapidfuzz

```

> Dependencies are already in the rootÂ test-automation/requirements.txt.
> 
> 
> You only need this if running YOLO separately.
> 

---

## ğŸ”‘ Environment Variables

Create aÂ `.env`Â file inÂ `test-automation/`Â or export directly:

```bash
export ROBOFLOW_API_KEY="your-roboflow-api-key"
export YOLO_API_PORT=8765   # optional (default: 8765)

```

---

## â–¶ï¸ Running the Server

From the root ofÂ `test-automation/`:

```bash
cd test-automation/yolo_detection
uvicorn yolo_api:app --host 0.0.0.0 --port 8765

```

Or with Python:

```bash
python -m yolo_detection.api

```

The server will be available at:

```
http://127.0.0.1:8765

```

---

## ğŸ“¡ API Endpoints

### Health Check

```
GET /health

```

**Response**

```json
{ "ok": true }

```

---

### Predict

```
POST /predict
Content-Type: multipart/form-data

```

**Parameters**

- `user_query`Â *(form)*Â â†’ natural language query (e.g.Â `"tap like button"`)
- `confidence_threshold`Â *(form, optional, default=0.90)*
- `image`Â *(file, optional)*Â â†’ PNG/JPEG screenshot
- `image_base64`Â *(form, optional)*Â â†’ base64-encoded image

**Example (cURL)**

```bash
curl -X POST "http://127.0.0.1:8765/predict" \
  -F "user_query=like" \
  -F "confidence_threshold=0.9" \
  -F "image=@screenshot.png"

```

**Success Response**

```json
{
  "ok": true,
  "match": true,
  "x": 512,
  "y": 1080,
  "cls": "like",
  "confidence": 0.95,
  "targets": ["like"],
  "latency_ms": 142
}

```

**Failure Response**

```json
{
  "ok": true,
  "match": false,
  "reason": "no confident match",
  "targets": ["like"],
  "latency_ms": 88
}

```

---

## ğŸ§© Integration

The main automation system (`test-automation/`) calls this service via HTTP at

`http://127.0.0.1:8765/predict`.

- If YOLO returns a match â†’ automation clicks the detected coordinates.
- If not â†’ fallback strategies are triggered (e.g., Qwen Vision).

---

## ğŸ“‚ Folder Structure

```
yolo_detection/
â”œâ”€â”€ api.py              # FastAPI app with /predict and /health
â”œâ”€â”€ yolo_integration.py # Python helpers for direct YOLO calls
â””â”€â”€ __init__.py

```

---

## ğŸ›  Development Notes

- Workflows are configured forÂ **TikTok UI elements**Â (`workspace_name="tiktok-qz4gk"`,Â `workflow_id="tiktokflutter"`).
- To adapt to other apps:
    1. UpdateÂ `YOLO_CLASSES`Â andÂ `INTENT_TO_CLASS`Â mappings.
    2. ReplaceÂ `workflow_id`Â inÂ `api.py`Â andÂ `yolo_integration.py`.