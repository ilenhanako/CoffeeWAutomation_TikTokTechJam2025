# â˜• Coffee with Automation

### AI-Powered GUI Testing for Real-World Mobile Apps

ğŸ“º **View product video & screenshots here:**  
ğŸ‘‰ [Devpost Submission](https://devpost.com/software/bombotest)


## ğŸš¨ The Problem
Modern GUI test automation isÂ **fragile**.

- **5â€“10%**Â of test code constantly breaks.
- **60%**Â of failures come from minor UI changes (e.g., a button moves, a popup appears).
- Current tools likeÂ **Appium, Espresso, XCUITest, Katalon**Â depend onÂ **brittle element locators**.
- Even newer AI approaches improve clicking flexibility â€” butÂ **fail to validate business goals**Â (like â€œDid the purchase actually complete?â€).

**Result:**Â Teams waste time fixing scripts instead of testing real functionality.


## ğŸ” The Gap
- Tools focus onÂ **element detection**, notÂ **end-to-end business validation**.
- Static locators/scripts break inÂ **dynamic UIs**Â (Android, iOS, Flutter, React Native, Hybrid).
- No resilience againstÂ **popups, ads, login walls, or app crashes**.


## ğŸ’¡ The Opportunity
Weâ€™re building aÂ **universal, vision-driven UI testing agent**Â that:

âœ… ConvertsÂ **natural language requirements**Â into test steps.  
âœ… UsesÂ **multimodal validation**Â (XML + vision + semantics).  
âœ…Â **Handles anomalies**Â gracefully with AI recovery.  
âœ… ValidatesÂ **business outcomes**, not just clicks.  

**Outcome:**Â Scalable, resilient, business-aware test automation thatÂ **evolves as your app evolves**.


## ğŸš€ What It Does
TheÂ **Coffee with Automation**Â framework transforms business goals into executable test scenarios:

1. **Knowledge Graph + RAG**
    - Turns vague requirements intoÂ **precise, validated test flows**.
    - Maps business terms (â€œupdate usernameâ€) to app components/actions.
2. **Appium + Qwen AI**
    - Executes steps inÂ **two modes**:
        - XML-based (fast, structured)
        - Vision-based (YOLO + Qwen-VL fallback)
    - Handles popups, dialogs, and interruptions automatically.
3. **Streamlit GUI**
    - Natural language input for test scenarios.
    - Run tests + viewÂ **logs, annotated screenshots, and results**.

## âœ¨ Key Differentiators
- **YOLO + Qwen-VL â†’ Icon-Aware Testing**: Detects icons directly (not just OCR text). Perfect for TikTok-style apps.
    
- **Developer-Curated Knowledge Graph**: Devs co-define UI flows â†’ agent tests withÂ **business context**, not guesses.
    
- **XML-First + Vision Fallback**: Reliable structured parsing + resilient visual grounding.
    
- **AI Evaluator + Recovery**: Detects if each step worked. Recovers from dialogs, ads, errors.
    
- **Feature-Oriented Testing**: â€œTest the Like buttonâ€ â†’ agent retrieves real user paths from the Knowledge Graph.


<details>
<summary>
<img src="https://img.shields.io/badge/âš¡_Why_It_Beats_AppAgentX_&_OmniParser-fff8c5?style=for-the-badge&labelColor=yellow&color=yellow" alt="Why It Beats AppAgentX & OmniParser"/>
</summary>

> **Better Perception, Better Planning, Better Reliability, Better Robustness, Better Relevance**

Other AI-driven test agents (like AppAgentX + OmniParser) showed whatâ€™s possible.  
We asked: **â€œWhatâ€™s missing for real-world mobile testing?â€**  
Hereâ€™s how we go beyond:


### 1. ğŸ–¼ï¸ Icon Detection with YOLO + Qwen-VL
- **Problem with AppAgentX:** Relies heavily on OCR + text reasoning â†’ fails on icon-only buttons.  
- **Our approach:** YOLO detects icons (even custom/unstyled ones) â†’ Qwen-VL interprets meaning â†’ Appium clicks it.  

âœ… Works for **TikTok-style UIs** where icons > text.  
âœ… Robust against design changes, dark mode, or missing labels.  

ğŸ’¡ *â€œWe replace fragile OCR perception with icon detection (YOLO) + semantic grounding (Qwen-VL).â€*


---

### 2. ğŸ§© Developer-Guided Knowledge Graph (Instead of Opaque AI Graphs)
- **Problem with AppAgentX:** AppAgentX **guesses the interaction graph dynamically**. â†’ devs canâ€™t guide or verify flows.  
    - It builds paths on the fly based on OCR/LLM interpretation.  
    - The result is opaque, unverified, and often wrong for edge cases.  

- **Our approach:** Developers define/edit the Knowledge Graph â†’ agent uses it to plan.  

âœ… Agent reasoning happens within a **verified structure**.  
âœ… Easier to adapt for new features.  
âœ… Paths reflect **real product logic** instead of one-off guesses.  
âœ… Graph doubles as **living documentation** of user flows.  

ğŸ’¡ *â€œWe let devs co-define interaction graphs â†’ agent tests with verified business logic, not blind guesses.â€*


---

### 3. ğŸ“± Mobile-Native Focus: XML First, Vision Fallback
- **Problem with AppAgentX:** Treats apps like static screenshots, ignores XML structure. Vision-only parsing is brittle.  
- **Our approach:**  
    - Use **Appium XML trees** when available (fast, reliable).  
    - Fall back to **YOLO + vision** only when XML fails.  

âœ… Optimized for **real mobile apps** (Android, Flutter, hybrid).  
âœ… Outperforms vision-only systems in production environments.  

ğŸ’¡ *â€œWe prioritize structured XML parsing â†’ fallback to vision only when needed.â€*


---

### 4. ğŸ¤– AI Step Evaluator + Interrupt Recovery
- **Problem with AppAgentX:** AppAgentX assumes progress until final step â†’ no mid-step validation, no recovery.  
- **Our approach:**  
    - **Evaluate each step** (did the UI update as expected?).  
    - Detect **interruptions** (dialogs, ads, errors).  
    - Auto-**recover** with dismissal, retries, or reroutes.  

âœ… Reduces flaky tests from random popups.  
âœ… Makes automation self-healing, not brittle.  

ğŸ’¡ *â€œWe added a step-by-step evaluator that confirms subgoals, spots interruptions, and dynamically re-plans.â€*


---

### 5. ğŸ¯ Feature-Driven Testing via Knowledge Graph
- **AppAgentX:** Treats a prompt (â€œLog in to Gmailâ€) as one giant guess â†’ builds a flat sequence, treats tasks as monolithic.  
- **Our system:** Retrieve realistic, developer-defined paths from the Knowledge Graph.  

ğŸ§ª Example: **â€œTest the Like Buttonâ€**  
- Path 1: Home â†’ Feed â†’ Scroll â†’ Like Button  
- Path 2: Notifications â†’ Post â†’ Like Button  

âœ… Covers **multiple real user journeys**, not just shortest paths.  
âœ… Captures **business relevance** (not just clicks).  
âœ… Automatically adapts when UI or flows change â†’ just update the graph.  

ğŸ’¡ *â€œWe donâ€™t just test clicks. We test features the way real users reach them.â€*


---

### 6. ğŸ” Structured Execution Loop (LangGraph)
- **AppAgentX:** Linear execution â†’ no mid-step checks, no recovery.  
- **Our system:** Every step follows **Perceive â†’ Execute â†’ Evaluate â†’ Recover â†’ Finish**.  

âœ… Ensures **each action is verified** before moving on.  
âœ… Guarantees recovery from errors, delays, or UI changes.  
âœ… Turns automation into a **deterministic loop**, not a gamble.  

ğŸ’¡ *â€œWe enforce a reliable execution cycle so every action is validated, recoverable, and stable.â€*

---


</details>

    

## ğŸ—ï¸ How We Built It
1. **Knowledge Graph + RAG**
    - Stores UI states, components, and flows.
    - Retrieval ensures contextually relevant tests.
2. **Qwen + Appium Execution**
    - XML-first interaction with AI-powered vision fallback.
    - AI evaluator checks subgoals, detects interruptions, and re-plans.
3. **LangGraph Execution Loop**
    - Every step follows:
        
        **Perceive â†’ Execute â†’ Evaluate â†’ Recover â†’ Finish**.
        
    - Ensures structured, resilient automation.
>
> ### ğŸ§ª Example
>**Prompt:**Â â€œTest the Like button feature.â€
>
>Our system:
>
>1. Finds realistic paths in the Knowledge Graph:
>    - Home â†’ Feed â†’ Post â†’ Like Button
>    - Notifications â†’ Post â†’ Like Button
>2. Builds execution steps (swipe, tap, verify).
>3. Executes withÂ **Appium + YOLO + Qwen-VL**.
>4. Evaluates: Did the Like action actually succeed?
>

âœ… Multiple real-world flows, resilient execution, business outcome validation.


## ğŸ† Accomplishments
- Recreated TikTokâ€™sÂ *For You*Â page inÂ **Lynx (ByteDanceâ€™s Flutter-like framework)**.
- Built aÂ **pipeline integrating KG + RAG + Qwen + Appium**.
- AppAgentX proved the concept;Â **we made it production-ready** and built aÂ **vision-aware, interruption-resilient automation agent**Â designed for **real-world test automation, production-ready**:
    - Vision + XML hybrid
    - Developer-guided graphs
    - Step evaluators + recovery
    - Feature-aware, business-relevant testing

**Result â†’**Â Tests thatÂ **scale with the app**, adapt to UI changes, and validateÂ **real user goals**.


## ğŸ“š What We Learned
- **Multimodal validation matters:**Â XML + vision beats XML-only or vision-only.
- **Context is king:**Â RAG + KG reduces ambiguity from LLM-only prompts.
- **Real-world readiness:**Â Recovery strategies are critical for production apps.


## ğŸŒ± Whatâ€™s Next
Our roadmap pushes even further:

- **Cross-Platform Expansion**Â â†’ Extend beyond Android to iOS and hybrid frameworks.
- **CI/CD Integration**Â â†’ Seamless hooks into pipelines for regression coverage.
- **Reinforcement Learning**Â â†’ Smarter anomaly recovery strategies that improve over time.
- **Collaborative Graph Editing**Â â†’ Teams co-create and refine the Knowledge Graph as living test documentation.
- **AppAgentX Integration**Â â†’ Today, AppAgentX tries toÂ guessÂ user flows by constructing graphs dynamically during execution.
> 
> Our vision is toÂ **replace guessing with grounding**: combine YOLO + Qwen for perception, and reason over aÂ **developer-defined Knowledge Graph**Â for explainable, reproducible testing.
>

## ğŸ“‚ Repository Structure (See Quick Start Instructions in each folder)
- **knowledge-graph/**Â â†’ KG + RAG for scenario planning
- **test-automation/**Â â†’ Appium + Qwen AI execution engine
- **yolo_detection/**Â â†’ YOLO FastAPI service for icon detection
- **tiktok-flutter-app/**Â â†’ Demo app for testing (built with Lynx/Flutter)
- **streamlit-gui-app/**Â â†’ Streamlit front-end for natural language testing


## â˜• Coffee with Automation
Building the future ofÂ **business-aware, vision-driven mobile test automation**.